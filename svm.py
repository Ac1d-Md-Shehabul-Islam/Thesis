# -*- coding: utf-8 -*-
"""SVM.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rH6_TfM3VzlKxDAdKTdr7Th5ZO8PjiDm
"""

import tensorflow as tf
tf.test.gpu_device_name()

import pandas as pd
# from sklearn.model_selection import KFold

df = pd.DataFrame(columns=["Label","Posts"])

chunksize = 5000
for chunk in pd.read_csv("/content/drive/MyDrive/Thesis Papers/SMHD/final_cleaned_data.csv", chunksize=chunksize):
  df=df.append(chunk,ignore_index=True)

from google.colab import drive
drive.mount('/content/drive')

df["Label"].value_counts()

df_c = df[df["Label"] == "control"]
df_d = df[df["Label"] == "ptsd"]

df_c = df_c.sample(frac = 1)
df_c = df_c.sample(frac = 1).iloc[0:3000,]  #control 3k
df_d = df_d.sample(frac = 1)  # 4471 ptsd
# df_d = df_d.sample(frac = 1).iloc[0:5000,]

# df_c = df_c.iloc[0:5000,]
# df_d = df_d.iloc[0:5000,]

"""# """

def stripstr(x):
  return len(x.strip())==0

result = pd.concat([df_c, df_d], ignore_index=True, sort=False)
empty = result[result["Posts"].apply(stripstr)]
indexarr = empty.index
result.drop(index = indexarr, inplace=True)

result = result.sample(frac = 1)

import re
import pandas as pd

import nltk
nltk.download('stopwords')

stopwords = nltk.corpus.stopwords.words('english')
wn = nltk.WordNetLemmatizer()
nltk.download('wordnet')

def tokenize(txt):
  tokens = re.split('\W+',txt) # W means non-word characters and + means one or more ["hello", 'my']
  return tokens

def lemmed(txt):
  text = [wn.lemmatize(word) for word in txt]
  return text

def remove_stopwords(txt):
  clean_msg = [word for word in txt if word not in stopwords]
  return clean_msg

result.describe()

result['Posts'] = result['Posts'].apply(lambda x: tokenize(x))
result['Posts'] = result['Posts'].apply(remove_stopwords)
result['Posts'] = result['Posts'].apply(lemmed)
result["Posts"]=result["Posts"].apply(lambda x: " ".join(x))

result

from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV

param_grid = {
    "C" : [10,80],
    "gamma" : [1, 0.01],
    "kernel" : ["rbf", "poly"]
}

from sklearn.feature_extraction.text import TfidfVectorizer
cv = TfidfVectorizer(max_features=4000)
X = cv.fit_transform(result["Posts"]).toarray() # all the contents

y=result["Label"]    #all the labels

Y = y.to_numpy() #numpy array representation of labels

x_train,x_test,y_train,y_test=train_test_split(X,Y,test_size=0.3,random_state=42)     # x_train is 70% trainig dta, y_train 70% t-label,

# def cut(sen, length = 3000):
#   content = sen.split()
#   if len(content) <= length:
#     return " ".join(i for i in content if i not in stop)

#   return " ".join(i for i in content[0:length] if i not in stop)

# filter1=df["Label"].isin(["control","depression"])
# df_cd=df[filter1]

"""# SVM"""

from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn import metrics

model = SVC(C=80, gamma = 0.01, kernel = 'rbf')# c=50, g=0.001, kernel=rbf pre=0.9

model.fit(x_train, y_train)

predictions = model.predict(x_test)

print(metrics.classification_report(y_test,predictions))

grid = GridSearchCV(model, param_grid, refit = True)

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 2.x
import tensorflow as tf
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

grid.fit(x_train,y_train)

# text_clf=Pipeline([("tfidf", TfidfVectorizer()),("clf", LinearSVC(random_state=0, tol=1e-5))])
# text_clf.fit(x_train,y_train)

predictions = text_clf.predict(x_test)

print(metrics.confusion_matrix(y_test,predictions))

print(metrics.classification_report(y_test,predictions))

print(metrics.accuracy_score(y_test,predictions))