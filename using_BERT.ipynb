{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "using_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxtdBYzzLsso"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4BBkk0taAKt"
      },
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Thesis Papers/SMHD/final_cleaned_data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0vzEx5lUaTdN",
        "outputId": "2cc4ad1c-6675-462d-b663-d8ee082836ed"
      },
      "source": [
        "df[\"Label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "control          15202\n",
              "depression       14139\n",
              "bipolar           6434\n",
              "ptsd              2894\n",
              "schizophrenia     1331\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yq0QqQ5S3nv3"
      },
      "source": [
        "df_c = df[df[\"Label\"] == \"control\"]\n",
        "df_d = df[df[\"Label\"] == \"schizophrenia\"]\n",
        "df_c = df_c.sample(frac =1 )\n",
        "df_c = df_c.sample(frac =1 ).iloc[0:1500,]\n",
        "df_d = df_d.sample(frac =1 )\n",
        "# df_d = df_d.sample(frac =1 ).iloc[0:5000,]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtjihcYD3-OP"
      },
      "source": [
        "def stripstr(x):\n",
        "   return len(x.strip())==0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CdRyKCSx5YCH"
      },
      "source": [
        "result = pd.concat([df_c, df_d], ignore_index=True, sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB9Gk0XL3-3r"
      },
      "source": [
        "empty = result[result[\"Posts\"].apply(stripstr)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYURJIeb4HFC"
      },
      "source": [
        "indexarr = empty.index\n",
        "result.drop(index = indexarr, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vy645h95wse"
      },
      "source": [
        "result = result.sample(frac = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxkmFj55xqG6"
      },
      "source": [
        "# def cut(sen, length = 1000):\n",
        "#   content = sen.split()\n",
        "#   if len(content) < length:\n",
        "#     return \" \".join(content)\n",
        "#   return \" \".join(content[0:length])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZLDu8cLjxF7i"
      },
      "source": [
        "# x=newdf['Posts'].apply(cut)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HwYG-DArdP5S",
        "outputId": "168935c0-74ff-4605-c292-ebf60fd31805"
      },
      "source": [
        "result[\"Label\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "control          1491\n",
              "schizophrenia    1330\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fr6-aJuc8eg"
      },
      "source": [
        "x=result[\"Posts\"]\n",
        "y=result[\"Label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIiv0b3WIF1U"
      },
      "source": [
        "y=result[\"Label\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfCJnjF_IAMZ"
      },
      "source": [
        "y = np.array(list(map(lambda x: 1 if x==\"schizophrenia\" else 0, y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6M87WgHAZ2pt"
      },
      "source": [
        "# bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti0NkbSIMrPS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "318a91d1-dbf9-424a-afcc-ef64cf5207aa"
      },
      "source": [
        "!pip install bert-for-tf2\n",
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bert-for-tf2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/a1/acb891630749c56901e770a34d6bac8a509a367dd74a05daf7306952e910/bert-for-tf2-0.14.9.tar.gz (41kB)\n",
            "\r\u001b[K     |████████                        | 10kB 23.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 20kB 18.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 30kB 11.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 40kB 9.3MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 5.5MB/s \n",
            "\u001b[?25hCollecting py-params>=0.9.6\n",
            "  Downloading https://files.pythonhosted.org/packages/aa/e0/4f663d8abf83c8084b75b995bd2ab3a9512ebc5b97206fde38cef906ab07/py-params-0.10.2.tar.gz\n",
            "Collecting params-flow>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a9/95/ff49f5ebd501f142a6f0aaf42bcfd1c192dc54909d1d9eb84ab031d46056/params-flow-0.8.2.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from params-flow>=0.8.0->bert-for-tf2) (4.41.1)\n",
            "Building wheels for collected packages: bert-for-tf2, py-params, params-flow\n",
            "  Building wheel for bert-for-tf2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bert-for-tf2: filename=bert_for_tf2-0.14.9-cp37-none-any.whl size=30535 sha256=6f023c9d9ae50f39328f4753fb0e5485775cfc7674fb3f18a1162065f298a8ab\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/04/ee/347bd9f5b821b637c76411d280271a857aece00358896a230f\n",
            "  Building wheel for py-params (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for py-params: filename=py_params-0.10.2-cp37-none-any.whl size=7912 sha256=439e964285c571fe865fecb676c21204f7b7d8f4475a5bfe7dfbe5956ed963c1\n",
            "  Stored in directory: /root/.cache/pip/wheels/d0/4a/70/ff12450229ff1955abf01f365051d4faae1c20aef53ab4cf09\n",
            "  Building wheel for params-flow (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for params-flow: filename=params_flow-0.8.2-cp37-none-any.whl size=19472 sha256=10384f57ff5bd7643ccd8a71297a75de049233b81f46ee7d744072457a27d3a8\n",
            "  Stored in directory: /root/.cache/pip/wheels/08/c8/7f/81c86b9ff2b86e2c477e3914175be03e679e596067dc630c06\n",
            "Successfully built bert-for-tf2 py-params params-flow\n",
            "Installing collected packages: py-params, params-flow, bert-for-tf2\n",
            "Successfully installed bert-for-tf2-0.14.9 params-flow-0.8.2 py-params-0.10.2\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_IcXzCEMy3E"
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "import bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSPFRnOQTLda"
      },
      "source": [
        "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
        "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\", trainable=False)\n",
        "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
        "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
        "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XzHwYpKTSSi"
      },
      "source": [
        "def tokenize_reviews(text_reviews):\n",
        "  return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvMNyLlHG20h"
      },
      "source": [
        "tokenized_reviews = [tokenize_reviews(review) for review in reviews]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hc-1Fa7KG46e",
        "outputId": "53a7f61f-b812-40b4-b9b6-4e94531fa546"
      },
      "source": [
        "type(tokenized_reviews[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0VhU04IWF-T"
      },
      "source": [
        "reviews_with_len = []\n",
        "i=0\n",
        "for  review in tokenized_reviews:\n",
        "  reviews_with_len.append([review, y[i], len(review)])\n",
        "  i=i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iS7LO-gCsJPJ"
      },
      "source": [
        "reviews_with_len.sort(key=lambda x: x[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUY43np1IfLz"
      },
      "source": [
        "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-YleFryWvvB"
      },
      "source": [
        "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzNSeebiW4pM"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xj7F1SeYW5LB",
        "outputId": "18fda9fc-be93-411e-c5c4-7aa6c4f2c22c"
      },
      "source": [
        "next(iter(batched_dataset))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(32, 491), dtype=int32, numpy=\n",
              " array([[ 6429,     0,     0, ...,     0,     0,     0],\n",
              "        [ 2204,  2678,     0, ...,     0,     0,     0],\n",
              "        [ 1054,  6806,  2050, ...,     0,     0,     0],\n",
              "        ...,\n",
              "        [ 8040, 29194,  3501, ...,     0,     0,     0],\n",
              "        [ 2130,  2488,  2084, ...,     0,     0,     0],\n",
              "        [ 3718,  2031,  2000, ...,  2096,  2017,  2064]], dtype=int32)>,\n",
              " <tf.Tensor: shape=(32,), dtype=int32, numpy=\n",
              " array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 0, 0, 0, 0, 0, 1, 0, 0], dtype=int32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oH3VCyZcZbzk"
      },
      "source": [
        "import math"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5Ol6EdCaYF4"
      },
      "source": [
        "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
        "TEST_BATCHES = TOTAL_BATCHES // 10\n",
        "batched_dataset.shuffle(TOTAL_BATCHES)\n",
        "test_data = batched_dataset.take(TEST_BATCHES)\n",
        "train_data = batched_dataset.skip(TEST_BATCHES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMPzYfDpaZ5E"
      },
      "source": [
        "class TEXT_MODEL(tf.keras.Model):\n",
        "    \n",
        "  def __init__(self,vocabulary_size,embedding_dimensions=128,cnn_filters=50, dnn_units=512, model_output_classes=2, dropout_rate=0.1, training=False,name=\"text_model\"):\n",
        "    super(TEXT_MODEL, self).__init__(name=name)\n",
        "    \n",
        "    self.embedding = layers.Embedding(vocabulary_size,embedding_dimensions)\n",
        "    self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,kernel_size=2,padding=\"valid\",activation=\"relu\")\n",
        "    self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,kernel_size=3,padding=\"valid\",activation=\"relu\")\n",
        "    self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,kernel_size=4,padding=\"valid\",activation=\"relu\")\n",
        "    self.pool = layers.GlobalMaxPool1D()\n",
        "    \n",
        "    self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
        "    self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "    if model_output_classes == 2:\n",
        "      self.last_dense = layers.Dense(units=1, activation=\"sigmoid\")\n",
        "    else:\n",
        "      self.last_dense = layers.Dense(units=model_output_classes, activation=\"softmax\")\n",
        "    \n",
        "  def call(self, inputs, training):\n",
        "    l = self.embedding(inputs)\n",
        "    l_1 = self.cnn_layer1(l) \n",
        "    l_1 = self.pool(l_1) \n",
        "    l_2 = self.cnn_layer2(l) \n",
        "    l_2 = self.pool(l_2)\n",
        "    l_3 = self.cnn_layer3(l)\n",
        "    l_3 = self.pool(l_3) \n",
        "    \n",
        "    concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
        "    concatenated = self.dense_1(concatenated)\n",
        "    concatenated = self.dropout(concatenated, training)\n",
        "    model_output = self.last_dense(concatenated)\n",
        "    \n",
        "    return model_output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Spq9SXwiagnS"
      },
      "source": [
        "VOCAB_LENGTH = len(tokenizer.vocab)\n",
        "EMB_DIM = 200\n",
        "CNN_FILTERS = 100\n",
        "DNN_UNITS = 256\n",
        "OUTPUT_CLASSES = 2\n",
        "\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "NB_EPOCHS = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlC_quXNakQz"
      },
      "source": [
        "text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
        "                        embedding_dimensions=EMB_DIM,\n",
        "                        cnn_filters=CNN_FILTERS,\n",
        "                        dnn_units=DNN_UNITS,\n",
        "                        model_output_classes=OUTPUT_CLASSES,\n",
        "                        dropout_rate=DROPOUT_RATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgcPtP95azNe"
      },
      "source": [
        "if OUTPUT_CLASSES == 2:\n",
        "  text_model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
        "else:\n",
        "  text_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"sparse_categorical_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHQ6fEvWazu8",
        "outputId": "3217e1a6-5334-444a-a902-55b06d2ded7e"
      },
      "source": [
        "text_model.fit(train_data, epochs=NB_EPOCHS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "     79/Unknown - 170s 2s/step - loss: 0.6540 - accuracy: 0.6217"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}